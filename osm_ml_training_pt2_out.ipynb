{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011328,
     "end_time": "2019-09-11T05:05:11.720609",
     "exception": false,
     "start_time": "2019-09-11T05:05:11.709281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AWS Sagemaker Training and Deploying\n",
    "## Cyclone Kenneth 2019-04-25\n",
    "### Part II\n",
    "\n",
    "In this part II notebook, we will upload the data to AWS S3 that we generated for training in the previous notebook. We will kick off an AWS Sagemaker object detection job and monitor the results. At the end of this notebook, you will have trained your own OSM-based CNN object detector!\n",
    "\n",
    "![](assets/happycloud.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005601,
     "end_time": "2019-09-11T05:05:11.732845",
     "exception": false,
     "start_time": "2019-09-11T05:05:11.727244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A couple of things worth noting:\n",
    "\n",
    "ðŸ¤” ML models are not super useful unless they are scaled across a large amount of data\n",
    "\n",
    "ðŸ¤” To effectively scale across data, you need to be efficient\n",
    "\n",
    "ðŸ¤” Because we will be passing sensitive data to this notebook in order to scale our cloud compute through Sagemaker, we will use papermill to run this notebook from within python. It creates a simple wrapper around the notebook so that we can specify variables.\n",
    "\n",
    "e.g.\n",
    "\n",
    "``` python\n",
    "import papermill as pm\n",
    "pm.execute_notebook('osm_ml_training_pt2.ipynb','osm_ml_training_pt2_out.ipynb', parameters = dict(sage_bucket='',my_bucket='', role=''))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 1.068596,
     "end_time": "2019-09-11T05:05:12.806877",
     "exception": false,
     "start_time": "2019-09-11T05:05:11.738281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005018,
     "end_time": "2019-09-11T05:05:12.819074",
     "exception": false,
     "start_time": "2019-09-11T05:05:12.814056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use 'papermill' (https://github.com/nteract/papermill) to pass sensitive variables to this jupyter notebook. Things like passwords, cloud locations, etc, should be paramterized as a best practice -- Never stored in a repo (especially public facing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.014558,
     "end_time": "2019-09-11T05:05:12.839012",
     "exception": false,
     "start_time": "2019-09-11T05:05:12.824454",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY=''\n",
    "SECRET_KEY=''\n",
    "sage_bucket=''         #this is the 'top-level' s3 bucket, in which you will have a team data-folder\n",
    "my_bucket=''           #this is the 'folder' where your sagemaker data lives\n",
    "prefix = my_bucket     #this is your model prefix\n",
    "sessname =''\n",
    "nclass = 1\n",
    "epochs =2\n",
    "mini_batch_size =2\n",
    "lr = 0.001\n",
    "lr_scheduler_factor =0.1\n",
    "momentum =0.9\n",
    "weight_decay =0.0005\n",
    "overlap = 0.5\n",
    "momentum = 0.45\n",
    "weight_decay =0.0005\n",
    "nms_thresh = 0.45\n",
    "image_shape =256\n",
    "label_width =600\n",
    "n_train_samples = 16551\n",
    "network ='resnet-50'\n",
    "optim = 'sgd'           #Stochastic gradient descent is an iterative method for optimizing an objective function      \n",
    "role = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.013192,
     "end_time": "2019-09-11T05:05:12.858810",
     "exception": false,
     "start_time": "2019-09-11T05:05:12.845618",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sage_bucket = \"eagleview-data\"\n",
    "my_bucket = \"team_echidna\"\n",
    "role = \"arn:aws:iam::649760770673:role/service-role/AmazonSageMaker-ExecutionRole-20190910T173949\"\n",
    "ACCESS_KEY = \"AKIAZOSGCRJY5YOTSRPR\"\n",
    "SECRET_KEY = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 6.068709,
     "end_time": "2019-09-11T05:05:18.932595",
     "exception": false,
     "start_time": "2019-09-11T05:05:12.863886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "my_east_sesison = boto3.Session(region_name = 'us-east-2',profile_name='uw')\n",
    "s3_client = my_east_sesison.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "s3 = my_east_sesison.resource('s3')\n",
    "\n",
    "s3_client.upload_file('rec/val.rec', sage_bucket, my_bucket+'/validation/val.rec')\n",
    "s3_client.upload_file('rec/train.rec', sage_bucket, my_bucket+'/train/train.rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.030085,
     "end_time": "2019-09-11T05:05:18.972131",
     "exception": false,
     "start_time": "2019-09-11T05:05:18.942046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(boto_session=my_east_sesison)\n",
    "training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 267.589755,
     "end_time": "2019-09-11T05:09:46.568315",
     "exception": false,
     "start_time": "2019-09-11T05:05:18.978560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-11 05:05:19 Starting - Starting the training job"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:05:21 Starting - Launching requested ML instances"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:06:16 Starting - Preparing the instances for training"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:07:22 Downloading - Downloading input data\n",
      "2019-09-11 05:07:22 Training - Downloading the training image"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:08:30 Training - Training image download completed. Training in progress."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'label_width': u'350', u'early_stopping_min_epochs': u'10', u'epochs': u'30', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'32', u'use_pretrained_model': u'0', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'', u'base_network': u'vgg-16', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'300'}\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'epochs': u'2', u'nms_threshold': u'0.45', u'optimizer': u'sgd', u'base_network': u'resnet-50', u'image_shape': u'256', u'label_width': u'600', u'lr_scheduler_step': u'3,6', u'momentum': u'0.45', u'overlap_threshold': u'0.5', u'num_training_samples': u'16551', u'mini_batch_size': u'2', u'weight_decay': u'0.0005', u'use_pretrained_model': u'1', u'num_classes': u'1', u'lr_scheduler_factor': u'0.1'}\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Final configuration: {u'label_width': u'600', u'early_stopping_min_epochs': u'10', u'epochs': u'2', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'2', u'use_pretrained_model': u'1', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'3,6', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.45', u'num_training_samples': u'16551', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'1', u'base_network': u'resnet-50', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'256'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Using default worker.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] nvidia-smi took: 0.0251889228821 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 WARNING 140001657984832] Training images are resized to image shape (3, 256, 256)\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 3 threads for decoding..\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 600\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 WARNING 140001657984832] Validation images are resized to image shape (3, 256, 256)\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 3 threads for decoding..\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 600\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Using [gpu(0)] as training context.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Create Store: device\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Using (gpu(0)) as training context.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:33 INFO 140001657984832] Start training from pretrained model 1.\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[05:08:33] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:34 INFO 140001657984832] Loaded pretrained model parameters.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[09/11/2019 05:08:38 INFO 140001657984832] Creating a new state instance.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1568178518.117609, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1568178518.11751}\n",
      "\u001b[0m\n",
      "\u001b[31m[05:08:38] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.47.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:08:56 Uploading - Uploading generated training model"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[09/11/2019 05:08:48 WARNING 140001657984832] Expected number of batches: 8275, did not match the number of batches processed: 10. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:48 INFO 140001657984832] #quality_metric: host=algo-1, epoch=0, batch=10 train cross_entropy <loss>=(0.942006098485626)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:48 INFO 140001657984832] #quality_metric: host=algo-1, epoch=0, batch=10 train smooth_l1 <loss>=(1.3595960330119174)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:48 INFO 140001657984832] Round of batches complete\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:49 INFO 140001657984832] Updated the metrics\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:49 INFO 140001657984832] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.007723679983414369)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:49 INFO 140001657984832] Updating the best model with validation-mAP=0.007723679983414369\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:49 INFO 140001657984832] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:49 INFO 140001657984832] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1568178529.53897, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 0}, \"StartTime\": 1568178518.117976}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:50 WARNING 140001657984832] Expected number of batches: 8275, did not match the number of batches processed: 9. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:50 INFO 140001657984832] #quality_metric: host=algo-1, epoch=1, batch=9 train cross_entropy <loss>=(0.844468781288634)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:50 INFO 140001657984832] #quality_metric: host=algo-1, epoch=1, batch=9 train smooth_l1 <loss>=(0.8480824206737761)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:50 INFO 140001657984832] Round of batches complete\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:50 INFO 140001657984832] Updated the metrics\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:51 INFO 140001657984832] #quality_metric: host=algo-1, epoch=1, validation mAP <score>=(0.03967245921411153)\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:51 INFO 140001657984832] Updating the best model with validation-mAP=0.03967245921411153\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:51 INFO 140001657984832] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:51 INFO 140001657984832] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1568178531.461606, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 1}, \"StartTime\": 1568178529.539223}\n",
      "\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:51 WARNING 140001657984832] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:52 INFO 140001657984832] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[09/11/2019 05:08:52 INFO 140001657984832] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"totaltime\": {\"count\": 1, \"max\": 19487.07103729248, \"sum\": 19487.07103729248, \"min\": 19487.07103729248}, \"setuptime\": {\"count\": 1, \"max\": 11.895179748535156, \"sum\": 11.895179748535156, \"min\": 11.895179748535156}}, \"EndTime\": 1568178532.752946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1568178513.341154}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-11 05:09:17 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 126\n",
      "Billable seconds: 126\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = 's3://{}/{}'.format(sage_bucket, my_bucket+'/train/')\n",
    "s3_validation_data = 's3://{}/{}'.format(sage_bucket, my_bucket+'/validation/')\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(sage_bucket, my_bucket)\n",
    "\n",
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p2.xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "                                         \n",
    "od_model.set_hyperparameters(base_network=network,\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=nclass,\n",
    "                             mini_batch_size=mini_batch_size,\n",
    "                             epochs=epochs,\n",
    "                             learning_rate=lr,\n",
    "                             lr_scheduler_step='3,6',\n",
    "                             lr_scheduler_factor=lr_scheduler_factor,\n",
    "                             optimizer=optim,\n",
    "                             momentum=momentum,\n",
    "                             weight_decay=weight_decay,\n",
    "                             overlap_threshold=overlap,\n",
    "                             nms_threshold=nms_thresh,\n",
    "                             image_shape=image_shape,   \n",
    "                             label_width=label_width,\t\t\n",
    "                             num_training_samples=n_train_samples)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "od_model.fit(inputs=data_channels, logs=True)    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02101,
     "end_time": "2019-09-11T05:09:46.603183",
     "exception": false,
     "start_time": "2019-09-11T05:09:46.582173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So now you are training!!! This will take a little while. We are only training for a very small number of epochs (2!), so we don't expect to have a really robust model. Potentially many 100s of epochs may be required depeneding on the quality and amount of training data we have. \n",
    "\n",
    "To level set, this model will be CRAPPY. But that is ok. You now have the basic tools required to set up and improve upon your own problem.\n",
    "\n",
    "ðŸ¤” What are the big considerations as a data scientist?\n",
    "\n",
    "ðŸ¤” What could we do to improve our model?\n",
    "\n",
    "ðŸ¤” How could we evaluate the quality of our data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 494.624306,
     "end_time": "2019-09-11T05:18:01.255067",
     "exception": false,
     "start_time": "2019-09-11T05:09:46.630761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    }
   ],
   "source": [
    "object_detector = od_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')   \n",
    "\n",
    "#response = object_detector.predict(data)\n",
    "\n",
    "# Tears down the SageMaker endpoint and endpoint configuration\n",
    "#object_detector.delete_endpoint()\n",
    "\n",
    "# Deletes the SageMaker model\n",
    "#object_detector.delete_model()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "duration": 771.040584,
   "end_time": "2019-09-11T05:18:01.868405",
   "environment_variables": {},
   "exception": null,
   "input_path": "osm_ml_training_pt2.ipynb",
   "output_path": "osm_ml_training_pt2_out.ipynb",
   "parameters": {
    "ACCESS_KEY": "AKIAZOSGCRJY5YOTSRPR",
    "SECRET_KEY": "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",
    "my_bucket": "team_echidna",
    "role": "arn:aws:iam::649760770673:role/service-role/AmazonSageMaker-ExecutionRole-20190910T173949",
    "sage_bucket": "eagleview-data"
   },
   "start_time": "2019-09-11T05:05:10.827821",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}